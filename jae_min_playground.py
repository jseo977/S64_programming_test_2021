# -*- coding: utf-8 -*-
"""Jae Min Playground.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1FreFHj4Bx8oTvaOAlacTJNhQ8Icw-fit
"""

# Install prerequisites
!pip install transformers

from transformers import BlenderbotTokenizer, BlenderbotForConditionalGeneration
mname = 'facebook/blenderbot-400M-distill'
model = BlenderbotForConditionalGeneration.from_pretrained(mname).to('cuda')
tokenizer = BlenderbotTokenizer.from_pretrained(mname)

# Check GPU Model
!nvidia-smi

tokenizer

# Initialise Variable
turn_separator = "    "

def get_token_len(input_str):
    return len(tokenizer([input_str], return_tensors="pt")["input_ids"][0])


# Trim transcript to length, without breaking conversation flow
def trim_transcript(transcript, persona = None):
    persona_str = ""
    persona_len = 0

    if persona is not None:
        # Create list of personas that has each persona
        persona = list(map(lambda x: "your persona: " + x, persona))
        persona_str = "\n".join(persona) + turn_separator
        persona_len = get_token_len(persona_str)

    conversation = turn_separator.join(transcript)

    # Remove until we are below limit
    while get_token_len(conversation) > 128 - persona_len:
        if len(transcript) < 1:
            break
        else:
            transcript = transcript[1:]
            conversation = turn_separator.join(transcript)

    return (transcript, tokenizer([persona_str + conversation], truncation=True, return_tensors="pt"))

PERSONA = ["i like cats", "I work at an artificial intelligence company"]
UTTERANCE = ["Hi"]

(transcript, inputs) = trim_transcript(UTTERANCE, PERSONA)
print(len(inputs['input_ids'][0]))
print(inputs['input_ids'[0]])
print(tokenizer.batch_decode(inputs['input_ids']))
inputs.to('cuda')
reply_ids = model.generate(**inputs)
print(tokenizer.batch_decode(reply_ids, skip_special_tokens=True)[0])

print(transcript)

import torch
torch.cat((inputs['input_ids'], inputs['input_ids']))

